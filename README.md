# w24-semantic-video-retrieval
Prototype for an intelligent video-archive retrieval system. Explores building a clean dataset from metadata and transcripts and testing semantic search with modern NLP models.


## A Prototype for Semantic, Person-Specific, and Emotion-Aware Search in Video Archives

### Overview

This project aims to design and evaluate a reproducible retrieval pipeline for searching within large video archives using video transcripts and associated metadata.
The core objective is to compare multiple sentence embedding models and determine how well they support semantic, person-specific, and emotion-based video retrieval.

Although the project may use real-world data (e.g., from a broadcasting archive), the research question and system design are kept general and independent of any specific company, ensuring full reproducibility and publishability.


### Motivation

Traditional video archives often rely on limited or manually created metadata, making it difficult to locate specific contentâ€”especially when queries involve:

- semantic meaning
- mentions of specific people
- emotional tone or mood

By developing a transcript-based retrieval pipeline, this project explores how modern embedding models and NLP techniques can significantly improve search capabilities and user access to archived audiovisual material.

### Goals

Build a prototype of an intelligent video retrieval system.

- Integrate semantic similarity, named-entity information, and emotion signals into the search process
- Compare the performance of different sentence embedding models
- Ensure the system is modular, scalable, and fully reproducible

### Dataset

The project is designed to work with:

1. Video Links
2. Video transcripts (generated by Mimir)
3. Metadata (timestamps, speaker names, topics)
4. Emotion labels predicted by text-based emotion classifiers
